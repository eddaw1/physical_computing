\documentclass[../physical_computing.tex]{subfiles}

\begin{document}

\chapter{Starting point}

\section{What is Physical Computing?}
\label{sec:what_is_physical_computing}

Science and Engineering all about making connections between nature and mathematics. A successful scientific theory makes a connection between the behaviour of some real world hardware in an experiment and the properties of an abstract mathematical system that reproduces what is seen in that experiment. The starting point is observations about the real world hardware, and the end point is a mathematical model exhibiting the same behaviour, and perhaps predictive power. In engineering it is the same process in reverse. You start with an abstract idea that you want a real world system to replicate, and your end point is the realisation of that real world system - a machine. So science and engineering are both about relationships between nature and mathematics. In order to do either effectively, it is sometimes best to start from the simplest building blocks. Conceptually, the simplest abstract system is one that can only be in two states, say true or false. A two state system like this is called a bit. Bits turn out to be the basic currrency of computers that are some of the most sophisticated machines that engineers have produced, and have started the information revolution. In this book, we will be mostly concerned at first with bits and how to represent them using electronic circuits. We will catch glimpses of what those electronic circuits can do for science, and how they can be used as tools for building better abstract models of reality, also known as scientific theories.

The manipulation of these representations of bits using circuits is called digital signal processing (DSP). In classical digital signal processing, ranges of voltages on wires are used to represent the values of bits. For example, in 3.3V CMOS logic, the convention is that any voltage between $\rm 2.0\,V$ and $\rm 3.3\,V$ represents logic $1$, or true, any voltage between $\rm 0.0\,V$ and $\rm 0.8\,V$ represents logic $0$, or false. Just as the bits themselves are represented by classical voltage ranges, the operations on the bits are represented by circuits. For example, the simplest abstract digital operation I can think if is NOT, where if the input is a $0$ then the output is a $1$ and vice versa. An implementation of a NOT in a digital circuit must generate an output between $\rm 0.0\,V$ and $\rm 0.8\,V$ from an input between $\rm 2.0\,V$ and $\rm 3.3\,V$, and vice versa. Though of course digital circuits have become incredibly sophisticated, we shall show that all of them are made up of three basic circuit elements - gates, registers and oscillators. So the fundamental building blocks of classical computers are fewer in number than the fundamental building blocks of the Universe in particle physics!

In the first part of this book, we will learn to design and build digital circuits out of these three basic building blocks. The tasks that can be carried out by these circuits will be very simple, but I hope that by understanding the basic principles of how these blocks are combined, you will be able to see how these ideas can be generalised to far more complex systems. Forty years ago you would have needed a soldering iron, breadboard, or wire wrapping tool to assemble your circuits. We will instead use a more modern approach, and that is to make use of a hardware description language (HDL). This language enables us to describe the properties of the system we would like to represent. We then use computer software that translates our hardware description language code into a file called a bitstream that is used to configure a flexible device called a field programmable gate array (FPGA). Once programmed, the FPGA implements the digital circuit that represents the abstract system we described with our HDL code. This FPGA chip can be obtained housed on a development board with a variety of inputs and outputs that allow us to verify that the digital circuit is faithfully representing the operation of the digital circuit we described in our HDL code. You will also learn how exist half way between the abstract system and its hardware representation by simulating the behaviour of the digital circuit before you actually program the hardware device. In industry, hardware description languages are also used to design more specialised chips such as application specific integrated circuits (ASICs) and even entire processor cores. So, learning how to program FPGAs with hardware description languages (HDLs) is a useful and transferable skill. 

In the second part of this book, we will learn how to make use of HDL code that others already wrote to describe sophisticated circuits combined with HDL code that we wrote ourselves. An analogy is in computer programming, where you rarely write every line of the code you are using yourself; instead you make use of libraries and routines that were written by others. In particular, we will implement a soft core processor called MICROBLAZE on our FPGA, and we will learn how to interface our own HDL code to the microblaze core. MICROBLAZE is a fully functional processor core, so in order to operate correctly it will need programming. We will learn to program our microblaze core in C, a high level computer language that is particularly suitable to working directly with digital hardware. We will learn the C programming that is needed for this task as we go along.

In the last part of the book, we consider an important generalisation of digital signal processing, called quantum computing. I said at the beginning that the basic currency of DSP is the bit, that can take two different values. What happens if we try and represent a quantum system using classical bits? For example, we might have a system consisting of an electron for which the z component of the spin can be either up or down, so that the states are $\Ket{\uparrow}$ and $\Ket{\downarrow}$. However, notice that in quantum mechanics $\left(\Ket{\uparrow}+\Ket{\downarrow}\right)/\sqrt{2}$ is also a possible state of this system. Therefore, classical bits are not suitable for representing the state of a quantum spin. This example leads us to the more general abstract concept of a qubit, a system that can be in any linear superposition of two orthogonal states. It turns out that qubits can be manipulated using quantum circuits to perform computing tasks that are impossible with classical digital signal processing. We shall in the last part of this course learn more about the building blocks of quantum computers, and some of the problems encountered in trying to realise them in practice.

In summary, this book is part practical and part theoretical. The practical aim is to teach you the fundamentals of hardware description languages and computer architecture. These are skills you can use later for very practical purposes, like getting a job. The theoretical part is acquiring some useful knowledge that is a little off the beaten track, particularly for pure scientists, and understanding where we might be going next. The world of digital signal processing is colliding as we speak with the world of quantum mechanics, so this is an exciting time to be alive.

\section{What is this course?}
\label{sec:whatisit}

A course at University is intended to take the student on a journey between some starting point, a body of knowledge and skills which it is presumed they already have, to some finish point, where the student has learned some new material with some intended purpose. Many courses make the mistake of assuming the starting point is no knowledge at all! This is the safest bet for an academic, but it has two immense disadvantages. First, ignoring the student's existing knowledge means you end up re-teaching material, costing you precious time. Second, the student's existing knowledge may be in some context that they have never questioned. Pointing out that there is a wider world out there that they have not explored may pique their interest and motivate them to devote energy and time to working in your course. An under-appreciated reality of teaching is that it is in part the art of seduction. Students will not work for you unless you can succeed in motivating them to do so.

This course will attempt to raise your enthusiasm level by pointing out that what you probably know about computing is almost certainly confined to an artificial and some would say Utopian environment. Learning how to do computing outside this environment, in the real `physical' world is liberating and fun. Furthermore, many of the applications of computers, or more generally of digital circuits and algorithms, underpin some of the worlds most interesting machines and experiments, from spacecraft to gravitational wave detectors. We will in this course explain the senses in which these statements are true, and we will learn some basic techniques in what I call `physical computing'.

\section{Analysis of a simple program}
\label{sec:projectforstudents}

Most of you will have done a course in computer programming using a high-level language such as PYTHON. Below is a code listing that I lifted from an online programming course for undergraduates.

\begin{minted}{python}
import numpy as np
from matplotlib import pyplot as plt

ys = 200 + np.random.randn(100)
x = [x for x in range(len(ys))]

plt.plot(x, ys, '-')
plt.fill_between(x, ys, 195, where=(ys > 195), \
facecolor='g', alpha=0.6)

plt.title("Sample Visualization")
plt.show()
\end{minted}
The program produces this graphical output.
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{chapter_1/figures/pythongraph.jpg}
    \caption{Graphical output of the PYTHON program}
    \label{fig:pythongraph}
\end{figure}

Let us analyse this program. First, its job is to make a plot of some random numbers. It probably isn't important exactly how long the code takes to produce this plot, as long as it isn't so long that it could have been done more efficiently without the computer. It also probably isn't critical that the program takes the same amount of time to produce this output every time it is run. 

The program makes use of some imported packages \texttt{numpy} and \texttt{matplotlib}, for mathematical functions and graphical output respectively.
It is somebody else's problem to implement these library functions. The \texttt{matplotlib} plotting function will only work if the computer has a way of outputting graphics. When I ran this code, the graphics appeared in my web browser, but this was because I was using Jupyter notebook; other python environments would have put the graphics in a separate window. 

Other observations about the program might seem obvious, but perhaps only because you have always taken them for granted. First, the program executes by default from top to bottom. Occasionally, there is a command that causes the code to go into a loop, so for example when the values of \texttt{ys} are generated, the
command \texttt{randn} generates 100 random numbers, so this random number 
generator loops around and runs 100 times before the program moves to the next
line of code. Similarly for the next line, which generates 100 values of \texttt{x}. All high-level programming languages feature loops, which cause code to execute multiple times. Another high-level programming concept is a conditional, meaning code that executes under some condition, but not if that condition is not satisifed. Loops and conditionals are so ubiquitous that you take it for granted that programs will utilise them. For that matter, you also take it for granted that code will execute from top to bottom.

Other things you might take for granted are particular to PYTHON. Notice how variables such as \texttt{x}, \texttt{ys} and \texttt{facecolor} represent data of different types, and that the type of data they represent is set automatically by PYTHON when the variables are defined. You don't generally have to declare what type of data a particular variable is going to be set to before that variable is used, PYTHON figures things out on the fly. In this respect, PYTHON differs from other high level languages like C or C++; in those languages you need to DECLARE a variable, including its data type, before you use it. You might think about whether this characteristic of figuring out variable types when they are first used is always a good thing, or whether perhaps it might occasionally cause problems.

The structure of high level computer languages generally comes about from them being intended for programming computers of a particular type. For example, most computers revolve around a very small number of processor cores which do all the calculations. Of course on a modern computer this may be less true than it used to be; there are often many cores on a CPU, and that doesn't include the GPU chip, where there are hundreds or thousands of cores that are more special purpose. In general, however, the architecture we imagine in a computer is that of a processor doing all the calculations, and input data and commands being piped in to this processor in sequence, and output form the processor coming out, so to speak, of the other side.

However, things do not have to be this way. In a more general calculating circuit, you can imagine the input data feeding in to the circuit through multiple parallel paths into a variety of different elements that all do things to subsets of the data at once. Such a circuit could be vastly more efficient than the gridlock that occurs when everything has to be stuffed in to a small number of processor cores. However, there is a price to pay, which is that the calculations in all these elements must be coordinated with each other, so that the outputs of the elements may combined and fed into yet more elements so that more calculations can be done, but on the right numbers. This way of doing things is potentially far more powerful than a conventional single processing unit architecture, but the programming of such a machine is vastly more complex as well. And, consider, the arrangement of the calculating elements for a particular computation may also need to be optimised for the efficiency of that computation. For the next computation, you may want to arrange the circuit differently.

So, in this course, we will consider the construction of digital circuits that do calculations. These circuits are literally wires connecting very fundamental building blocks. Until the 1980s. digital circuits were assembled using literal physical wires or circuit boards, connecting these fundamental blocks together. If you wanted a new circuit, you had to build it out of hardware. In the 1980s, however, large scale integration of digitial hardware led to a more abstract way of doing things. You described the digital circuit that you wanted using a new type of language, a Hardware Description Language (HDL). By a wierd stroke of luck I programmed early circuits of this type, which were called PALs, or programmable arrays of logic, using an early HDL called ABEL, when I was doing work experience at school. I had no idea that I would wind up using it's direct descendent, called VHDL, a far more advanced hardware description language, in my physics research more than 35 years later. Life is strange sometimes. Anyway, the hardware description language literally permits you to build a digital machine. You are building hardware, using a language.

Having built the hardware, in simple cases, you just pipe the data in, and the desired output appears at the other end of your digital machine. This has two great advantages. First, it is extremely fast. There is no operating system or other environment to slow down the process. Second, often the calculation takes exactly the same amount of time every time it is executed. This makes this type of machine suitable for use in situations where the output of the calculation is used to control the state of something critical, like a physics experiment, where unexpected delays cause glitches which may cause the machine or experiment to stop working.

In more complex cases, your machine may grow to resemble a computer, and you may need to write more code, this time perhaps in a high level language, to program it. But because you have far more control over the architecture of your machine than yo do with an ordinary computer, you are empowered to make a far more interesting, useful and powerful calculation engine. I hope you will enjoy learning physical computing, as I have enjoyed it. It's physics, but not perhaps as you have experienced it before.

\section{Lab Number 1. Getting to know the Development Hardware}
\label{sec:exploring_our_hardware}

We will develop our hardware and software in an integrated development environment called Vivado/Vitis, and deploy to a development board. The development environment will be running on your loaned Lenovo ThinkPad i7 PC and the test hardware consists of a BASYS3 development board. These are connected together with a USB to micro-USB cable. 

The Lenovo ThinkPad runs Ubuntu, a dialect of the LINUX operating system, which is an example of the UNIX family of operating systems that has been around since the mid 1970s. The choice of LINUX rather than Windows is for many reasons, but here I highlight one of them. Microsoft has a habit of inducing Windows to install updates at random times. Experience taught us that too often students were sitting in the lab waiting for a windows update to complete, and there was no obvious way to stop this happening. In UNIX, updates are very much under the control of the administrator of the system. In the case of your laptops this is Mitch, and we only update in the off-season! There are other reasons but this is the main one.

The PCs are quite nice and in particular quite fast because they have several intel i7 processor cores, and the software we are running to program the BASYS3 boards can make use of these in parallel. They do have some annoyances. The main one is the rather small screen. If at home you have an HDMI monitor and cable, then you can use an ordinary HDMI cable to connect the laptop to an external screen, which makes using the computers more comfortable. You can also plug in your own mouse via USB. The other annoyance is the ‘PgUp’ and ‘PgDn’ (page up and page down) buttons being located right next to the cluster of arrow buttons on the keyboard, which means that sometimes you find yourself taking forays into bits of your code you didn’t intend to visit. There is nothing that can be done about this; I only flag it in case you don’t understand what is happening, and advise you to be conscious of exactly where your finger is when trying to use the arrow keys. You can use a USB mouse with the laptops - just plug it in and it should start working. One with a 'wheel' on it can be particularly useful for scrolling up and down code and documents, like this book.

The BASYS3 boards are designed for teaching purposes are are therefore easy to use relative to other development boards we have tried. However, the hardware on these boards is quite cheap, and occasionally fails. Please do not leave the connecting cable plugged in to the USB port on the board when you put it in your bag to carry it around! This is a great way to destroy the rather fragile USB connector on the board. The other failure point is the 16 LEDs and switches along the bottom edge of the board, below the BASYS3 logo. The switches wear out quite quickly, and the LEDs (rather surprisingly) also fail quite often. In todays project we will check all the switches and LEDs on your boards.

You will all be learning to use LINUX, which many of you will never have met. This should not be a handicap on the course. An important difference between LINUX and Windows is that in LINUX the command line interface, accessed by opening the terminal program, is a very powerful environment. Windows users mostly abandoned the DOS prompt and direct issue of commands to the operating system many years ago, although amongst engineers, scientists and developers DOS still does get used, in spite of its many shortcomings. In LINUX it is common to make use of commands issued directly into a terminal for tasks like creating directories, moving files, and communicating with other machines on the internet using various communication protocols (ssh, sftp, git and other things that we shall learn about). You’ll find yourself using the terminal window to get things done. This is a useful transferable skill into many lines of work and post degree research.

The laptops have all been set up with the same environment and software. You will find that you do not have permission to install your own software on the system or make major changes to the way your computer is set up. It is not in your interest to make major changes to the system -  for example trying to repartition the disk and install a windows operating system on a separate partition. We therefore make a rule against attempting such hacks, and Mitch will not be happy, and will tell me (and I will be even less happy) if it becomes clear that somebody has broken this rule. 

One final point. This is a course that teaches practical skills. As with other practical skills – riding a bike, playing a musical instrument, etc, real facility comes with practice. The labs give you a total of about 33 hours of time officially dedicated to this. However, because I am lending you the hardware, you can also practice outside labs, and are strongly encouraged to do so. In about 5 years of teaching the course, several of our students have used skills gained on the course to get very good jobs in companies. I am very much hoping to have one of them talk to you all about their experience in industry using things that we taught on this course. The course is largely what you make of it. If you work hard, you are gaining some genuinely useful skills for your CV and future career, if that is what you decide you want.

\section{Brief Tour of the Laptop}
\label{sec:brieftour}

The first job is to turn on the laptop, and log in under the physics user. Your username will remain ‘physics’ for the duration of the course.

The first job is to connect the laptop to the campus wifi. It will not connect by default – Mitch has wiped the hard drive and it is essentially equivalent to a fresh-out-of-the-box system. 

With the exception of the very first instruction, the instructions below are identical to the University’s seen at \url{https://students.sheffield.ac.uk/it-services/wireless/connect#linux}. These are to be followed with one exception. The root certificate download can be found on the laptop already at \\ \texttt{/etc/ssl/certs/QuoVadisOVRootCertificate.crt} :-

\begin{enumerate}
\item{Click on the downwards arrow in the top right hand corner of the desktop and select `Wifi Not Connected' then `Select Network', then select eduroam, then press the `Connect' button at the bottom of the selection window.}
\item{{\it NOTE - you do not have to follow this instruction because the file is already on your machine at the path above.} Download the QuoVadisOVRootCertificate.crt certificate to your machine from \\
\url{https://www.sheffield.ac.uk/polopoly_fs/1.950471!/file/QuoVadisOVRootCertificate.crt}.\\
}
\item{When in range of eduroam click on it to enter settings.}
\item{Use the following settings:
\begin{itemize}
\item{Wireless Security: WPA2 Enterprise}
\item{Authentication: PEAP}
\item{CA Certificate: Browse to where you saved the certificate and select}
\item{Inner Authentication: MSCHAPv2}
\item{Username: Your UoS username followed by @sheffield.ac.uk}
\item{Password: Your UoS password}
\end{itemize}
You should then be able to connect to eduroam}
\end{enumerate}

Figure \ref{fig:emptyscreen} shows the Ubuntu window manager you should see when you have logged in. Down the left hand side there are a set of icons in a toolbar that run different pieces of software. Yours are not quite the same as mine as I have added a few extras. On your machines, there are seven that will prove useful during the course. By hovering with your mouse pointer over the icon you can see what these applications are. Locate the following applications:

\begin{enumerate}
    \item Firefox Web Browser
    \item Files
    \item Terminal
    \item Serial port terminal
    \item Vivado 2019.2
    \item Xilinx Vitis IDE 2019.2
    \item Documentation Navigator
\end{enumerate}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/blank_ubuntu_screen.png}
    \caption{A blank screen upon logging in as physics to Ubuntu}
    \label{fig:emptyscreen}
\end{figure}

You can single-left-click on any of these items to run the corresponding applications. If you run something and want to stop, just as in Windows, use the red 'x' in the top right hand corner of the window. Most windows can be resized by dragging on the corners, minimised by clicking on the underscore also in the top right corner, and expanded to full-screen mode by clicking on the rectangle in the same place. If you minimise a window belonging to one of the sidebar applications, it can be recovered by clicking again on the icon. You can also re-open a minimised icon by clicking on 'Activities' in the top left hand corner. The full set of installed applications can be seen by clicking on `nine dots' icon in the lower left corner. You can drag across the mouse pad to go up and down the full set of icons. Clicking on it again takes you back. If you are alone and want some background music or talk, the 'RythmnBox' application plays podcasts. 

The two icons in the top left of the main window are short-cuts to the Files application running on your home directory and the contents of the rubbish bin

Firefox should need no explanation. The `Files' application is like the Windows file manager, and it allows you to navigate through the hierachy of folders on the Ubuntu machine. If you right click on a file you are given an option to move to the recycle bin, just as in Windows. In fact, Ubuntu is trying to emulate the Windows operating system. The big difference between Linux and Windows is the use of the Terminal application. Windows has the DOS prompt, but DOS is so arcane that few but the brave use it very much. In Linux the Terminal 'shell' is the BASH shell, which has quite a lot of functionality and is quite sophisticated. However, you shouldn't actually need to learn too much UNIX/Linux to do this course. If you are interested, there are lots of tutorials on line in learning the BASH shell in UNIX/Linux. The serial port terminal will be useful later in the course when we are communicating with microprocessor cores on the BASYS3 board. Vivado and Vitis are the two graphical user interfaces that we will use to develop our applications. We will spend most of our time using Vivado and Vitis.

Before you can use Firefox you will need to connect your laptop to a wireless network. The icon to do this is in the top right hand corner of the screen and looks like a segment of fruit. If you click on this icon it will show you a list of available wireless networks - you should check and see that there is one that you can log in to. After logging in, the segment will be partially filled in, indicating the quality of your connection.

You should be able to use Firefox to log on to MUSE and from there with the google mail tool and Blackboard you should be able to use your laptop to take part in the lab projects without need for your own personal devices. The camera built in to the laptop and the built in microphone both work with blackboard and we will need to communicate with each other freely to make a success of the course, within the limits imposed by bandwidth.

To shut the laptop down, the small downwards arrow in the very top right pulls down a menu from which you can select the Power Off/Log Out option, and there you can do all the usual things - turn off, restart, log out, etc. In these things Ubuntu again behaves much like other popular operating systems.

There are also several other equipment items. These are, all told
\begin{enumerate}
    \item Cardboard box and packing material (please preserve in good condition)
    \item Laptop with power supply (two connected cables)
    \item Gel filled protective laptop case
    \item USB to micro-usb adaptor cable
    \item Basys3 FPGA development board
    \item PMODDAC2 two channel digital-to-analog converter (DAC) - this is small!
    \item some cables
    \item a crystal earpiece
\end{enumerate}

Obviously try not to lose anything and tell me if you do, or if anything gets broken. Try NEVER to keep drinks anywhere they can spill on the laptop. Especially on the table directly next to them. Think of them as lab equipment. You wouldn't drink in a lab, so don't keep drinks in the vicinity of this machinery.

One final thing about the laptops. If you want to dump the whole screen to a picture file, then at any time you can just press the PrtScr button which is on your keyboard two keys to the right of the space bar, and a `.png' format bitmap of the entire screen is written to the Pictures subfolder of your home directory. Try this, and then use the Files application to go to this directory, double clicking on the icon to view it. If you just want to print one window, make sure this window is selected, then hold down the Alt key and again press PrtScr (Alt-PrtScr), and a `.png' file just of the highlighted window is put in the same place. This could be useful if something isn't working right and you want to send myself or Mitch graphics that are useful in explaining your problem.

\section{Logic Gates}
\label{sec:logicgates}

We start our physical computing course where all beginners courses on digital circuits begin, with logic gates.
A logic gate is an abstract operation having a single digital output and one or more digital inputs. There are seven
logic gates that you will see on schematics. The simplest of them is a NOT gate, or inverter, mentioned in the preface,
whose output is $1$ when its single input is $0$, and $0$ when the input is $1$. The next simplest gates each have two inputs. The two input AND gate has output $1$ only when both its inputs are $1$, the two input OR gate has output $1$ when either or both of its inputs are $1$, and the two input XOR (exclusive or) gate has output $1$ when either one, but not both, of its inputs are $1$. That's four so far. The other three gates consist of the AND, OR and XOR gates with an inverter on the output, forming the NAND, NOR and XNOR gates. The six two-input gates generalise to more than two inputs. For example, a 3 input XOR gate has output $1$ when any one, but not more than one, of its inputs is $1$.

An alternative to these written descriptions of functionality is to tabulate the outputs for each possible combination of inputs. Such tables are called truth tables. The truth tables for the four gates NOT, AND, OR and XOR are shown in Figure \ref{fig:logic_gates_figure}
along with the symbols representing the gates. The figure caption tells you how to draw the other three gates NAND, NOR and XNOR. 

\begin{figure}[t]
\centering
\includegraphics[width=0.5\textwidth]{figures/logic_gates_figure.pdf}
\caption{\label{fig:logic_gates_figure} Truth tables and symbols for four of the seven logic gates commonly encountered. The other three, NAND, NOR and XNOR are obtained by adding a not at the output of AND, OR and XOR, respectively. The symbols are the same as for AND, OR and XOR except for the addition of a small circle like that on the right side of the NOT gate. Sometimes you will also see a small circle on one of the input leads before the gate body. For example, if this circle before the body of the AND gate appears on the b input, it means `a and not b.'}
\end{figure}

\section{De Morgan's Laws}
\label{sec:demorgan}

It turns out that you can build AND gates out of OR gates and NOT gates, or OR gates out of AND gates and NOT gates, as a consequence of a useful piece of maths that comes from the theory of sets, called De Morgan's laws. In the theory of sets, if $A$ and $B$ are each sets, then $A\cap B$ is called the intersection of $A$ and $B$, and means all the elements that are in both set A and set B, and $A\cup B$ is called the union of $A$ and $B$ and means all the elements that are set A or set B. So there is a direct correspondence between the logical $\rm AND$ and the intersection, and between the logical $\rm OR$ and the union. What about $\rm NOT$? In set theory the symbol for all the elements not in set $A$ is the complement of $A$, or $\overline{A}$. There are two De Morgan's Laws, and both can be stated either in set notation or in logic notation. The logical statements are

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.45\textwidth]{figures/venn_diagram_figure.pdf}
    \caption{\label{fig:venn_diagram_figure} Venn diagrams illustrating De Morgan's laws.}
\end{figure}

\begin{align}
\neg(a\&b) &= \neg a | \neg b
\label{eq:demorganone} \\
\neg(a|b)&=\neg a \& \neg b.
\label{eq:demorgantwo} \\
\nonumber
\end{align}
In words, Equation \ref{eq:demorganone} states that not(a and b) is the same as (not a) or (not b), and Equation \ref{eq:demorgantwo} states that not(a or b) is the same thing as (not a) and (not b). The same two laws can be stated in set notation as
\begin{align}
    \overline{A\cap B}&=\overline{A}\cup\overline{B}
    \label{eq:demorganoneset} \\
    \overline{A\cup B}&=\overline{A}\cap\overline{B}.
    \label{eq:demorgantwoset} \\ \nonumber
\end{align}
If you like Venn diagrams, you can prove De Morgan's Laws with them. This is done in Figure \ref{fig:venn_diagram_figure}. The rectangle represents all objects in some space, everything inside the left hand ellipse is in set A, and everything inside the right hand ellipse is in set B. The complements of $A$ and $B$ are shown in the second row. The union and intersection of $A$ and $B$ are in the third row. The fourth row shows the complements of the union and the intersection of $A$ and $B$. The union of $\overline{A}$ and $\overline{B}$ is the areas that are blue in either of the diagrams in the second row. This is everything except the thin white sliver common between both $A$ and $B$, in other words it is the complement of the intersection of $A$ and $B$. This proves De Morgan's first law, Equations \ref{eq:demorganone} and \ref{eq:demorganoneset}. The intersection of $\overline{A}$ and $\overline{B}$ is the areas that are blue in both of the diagrams in the second row. This is everything that isn't in either of the white ellipses of $A$ or $B$, in other words it is the complement of the union of $A$ and $B$. This proves De Morgan's second law, Equations \ref{eq:demorgantwo} and \ref{eq:demorgantwoset}.

De Morgan's laws simplify the job of chip designers since essentially all of the logic gate elements on a chip may be fabricated out of either OR gates or AND gates, plus inverters. Figure \ref{fig:de_morgan_logic} is a third statement of De Morgan's laws in terms of the symbols for the gates themselves.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.2\textwidth]{figures/de_morgan_logic.pdf}
    \caption{\label{fig:de_morgan_logic} De Morgan's laws in terms of the equivalence between different types of logic gate.}
\end{figure}

\section{Parallel Buses}
\label{sec:parallel_buses}

We aren't going to get very far if we only ever deal with one bit at a time. The simplest way to streamline manipulations of multiple bits is to perform the same operation on many bits in parallel. A collection of bits in parallel is called a bus, and the number of bits in the bus is called the bus width. Frequently buses consist of 8, 16 or 32 bits in parallel. There are other kinds of buses other than parallel, and we will discuss some of these presently, but for now just imagine an assembly of signals in a set of wires. Buses are ordered. In other words, the position of a particular bit of a bus is well defined. Usually we think of a bus as an ordered set of bits from left to right, with the rightmost bit called the least significant bit, or LSB, and the leftmost bit called the most significant bit, or MSB. This is in preparation for use of parallel buses to represent integers, which we will discuss in Section \ref{sec:logic_and_integers}. 

The logic operations defined in Section \ref{sec:logicgates} were so far discussed as operating on a single bit, but they can also operate on buses containing many bits, so long as each input and output has the same bus width. Used in this way, the logic operations NOT ($\neg$), AND ($\&$), OR ($|$), XOR ($\oplus)$, NAND ($\neg\&$), NOR ($\neg|$) and XNOR ($\neg\oplus$) are examples of `bitwise' operations, in that they operate on each bit separately and do the same thing to all the bits. So, for example,
\begin{align}
    \neg\,B00010000&=B11101111
    \label{eq:first_bus_operation}
\end{align}
 and
\begin{align}
   B10101010\,|\,B00010000\,&=\,B10111010.
   \label{eq:second_bus_operation}
\end{align}
 Here I have used the notation of a leading $B$ to mean that the sequence of digits following represents a bus of bits - just a way of expressing whether the bits in a bus are high or low, $1$ or $0$, or equivalently true or false.
 
 There are other binary operations that can only meaningfully act on parallel buses of bits. Bit shifting operations, in particular, involve shifting the values in all the bits in a bus to the left (in the direction of the most significant bit) or to the right (in the direction of the least significant bit). As a consequence, either the rightmost or the leftmost bit falls off the end of the bus and is lost, and a gap appears at the other end. In a logical bit-shift operation, the gap is filled with a zero. In an arithmetic bit shift, the gap is filled with a copy of the value in the bit that must moved along to create the gap. Finally, you could possibly place the value in the bit that fell off the other end of the bus in this position. All three types of bitshift operations can sometimes be needed. Where the bus is representing logical rather than arithmetical data, logical bit shifting is the most common. An exercise in Appendix \ref{sec:logic_in_vhdl} will show you how to execute logical bit shifts on buses representing logical data in the hardware description language VHDL.
 
 The use of binary operations to simplify and speed up computer code is an interesting subject. There is a nice book by Warren \cite{Warren:10.5555/2462741} where you can read about logical consequences of De Morgan's laws and mathematical tricks involving bitwise logic operations combined with bit shifts, particularly in Chapters 1 and 2, which I will make available to you on Collaborate.

\section{Logic and Integer Arithmetic}
\label{sec:logic_and_integers}

\subsection{Logic and the Foundations of Mathematics}
\label{sec:logicandmathsfoundations}

De Morgan's laws are an example of the connection between the theory of sets and logic. This is natural because both sets and logic are closely connected to the foundations of mathematics. The drive to build a logical foundation for mathematics was an area of intense activity at the turn of the 20th century. The famous series of books, Principia Mathematica, by Russell and Whitehead \cite{Whitehead:268025}, an attempt to build a consistent and complete logical foundation for mathematics, famously collided with the philosophy of Kurt G\"odel who proved that the task is impossible. Modern formulations of axiomatic set theory, or the logical foundations of mathematics, are based on Zermelo-Fraenkel set theory, and it is in this context that modern developments in the logical foundation of mathematics are usually discussed. The subject of the logical foundations of mathematics is fascinating. If you are interested, and there are many popular books on the subject which will stretch your mind, especially if you try and do the puzzles \cite{Hofstadter:99665,Smullyan} !

Fortunately we do not need to construct a complete logical basis for mathematics, only to work out some ways of using logic gates to do simple arithmetic, which is much easier. Nonetheless, we shall quickly discover that in using logic to do mathematics you have to be careful to avoid encountering difficulties. These difficulties shall be discussed in Chapter \ref{sec:registers}.

\subsection{Binary Representation of Positive Integers}
\label{sec:postiveintegers}

Let us first consider representation of integers. Bits naturally lead to the use of binary numbers to represent integers. Any sensible representation will have the integer zero represented by any number of $0$s, and the integer one represented by a single $1$. If the integers are all positive or zero, then all we need is ordinary binary numbers, so the numbers 0 to 15 can be represented by three binary digits. In ascending order, $B0000$, $B0001$, $B0010$, $B0011$ $B0100$, $B0101$, $B0110$, $B0111$, $B1000$, $B1000$, $B1001$, $B1010$, $B1011$ $B1100$, $B1101$, $B1110$, $B1111$. If we want to represent any larger number than 15, we will need more bits in our binary representation than four. The largest unsigned integer representable using an $N$ bit parallel bus is $2^N-1$.

\subsection{Hexadecimal Notation}
\label{sec:hexadecimal}

I am already getting tired of writing all those zeros and ones, and in digital signal processing it is common to represent collections of binary numbers using hexadecimal, or base 16. Because people commonly have ten figures, we have developed naturallly to have symbols for each of the counting numbers between 0 and 9, and think of larger numbers as groups of 10, or groups of 100, and so on. However, suppose we had actually had 16 fingers! We would then have naturally developed separate symbols for the first 16 integers 0 to 15 that are also each representable by four binary bits. Table \ref{tab:hexadecimal} shows the first 16 numbers from the set of positive integers and zero, together with their decimal and hexadecimal representations.

\begin{table}[hbt]
    \centering
    \begin{tabular}{|c|c|c|}
    \hline
         decimal & binary & hexadecimal \\
         \hline\hline
         0 & B0000 & 0x0 \\
         1 & B0001 & 0x1 \\
         2 & B0010 & 0x2 \\
         3 & B0011 & 0x3 \\
         4 & B0100 & 0x4 \\
         5 & B0101 & 0x5 \\
         6 & B0110 & 0x6 \\
         7 & B0111 & 0x7 \\
         8 & B1000 & 0x8 \\
         9 & B1001 & 0x9 \\
         10 & B1010 & 0xa \\
         11 & B1011 & 0xb \\
         12 & B1100 & 0xc \\
         13 & B1101 & 0xd \\
         14 & B1110 & 0xe \\
         15 & B1111 & 0xf \\
        \hline
    \end{tabular}
    \caption{Decimal, binary and hexadecimal representation of zero and the first fifteen positive integers.}
    \label{tab:hexadecimal}
\end{table}

The hexadecimal notation is extensible to numbers represented by many more bits. For example, on computers integers are commonly represented as 32 bit binary numbers, so that the unsigned integer 1 would be represented as $\rm B00000000000000000000000000000001$. In hexadecimal this is $\rm 0x00000001$, which is still a lot of leading zeros, but you can see how the reduction in the number of symbols by a factor of four when using hexadecimal notation instead of binary is attractive.

\subsection{Binary Counters and Oscillators}
\label{sec:oscillators}

While we are looking at this table, it makes obvious another useful feature of binary numbers, and that is the behaviour of the binary digits. Scan down the column of binary numbers and look only at the least significant bit (LSB). Notice that as the count proceeds, the least significant bit oscillates back and forth between $0$ and $1$ with a period of two counting numbers. Now look at the second-least significant bit. This bit also oscillates back and forth between zero and one, but the period of this oscillation is four counts. The third least significant bit also oscillates periodically with a period of 8 counts, and the most significant bit (MSB) has a period of the full 16 counts. This means that if we can cause a bus to count upwards from zero, each subsequent bit can serve as an oscillator with a period of twice that of the previous bit. It turns out to be very useful indeed to be able to make oscillators with different periods in digital devices, and we shall see the power of this in Chapter \ref{sec:registers}.

\subsection{Overflows in Binary Arithmetic}
\label{sec:overflows}

We have already realised that we need more than four bits to represent a positive binary number greater than 15. What is the convention for dealing with overflows? What happens when you add 1 to 0xf ? We will operate with the following convention, following digital signal processing conventions: {\it overflows are disregarded}. If you require more bits than you have designed into your system to get the right answer, it's just tough - you should have used more bits. While this might sound harsh, in fact the convention that overflows are ignored is very useful indeed.

The first reason for this is to do with counters. As alluded to before, if we count up in binary, each successively more significant bit behaves like an oscillator with half the period of the bit to the right. What about the most significant bit? What does it do? It starts out as 0, then half way to the maximum capacity of the bus, it switches to 1. What happens when it reaches full capacity? This is when all the bits are $1$. The next number in the sequence, {\it ignoring the overflow} is all zeros. In other words, the counter just resets to zero, and then keeps counting. A counter that starts at zero, then gets to the maximum number that can be represented with the number of bits, then resets to zero automatically and starts again, is very useful indeed!

There's another very useful application of ignoring overflows. It means that a binary number of N bits containing all 1s is adjacent in the sequence of binary numbers to a binary number containing all 0s. This comes in very handy when representing negative integers with a system called {\it twos complement} which we will discuss in the next section.

To summarise this section, in a binary representation of numbers, overflows are ignored. If you have 4 bits, then all numbers are represented using 4 bits, and if you end up running off the end, you neglect any overflows and instead your sequence wraps around back to the other end.

\end{document}